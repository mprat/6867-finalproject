\documentclass[10pt]{article}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
% \usepackage{multirow}
% \usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{cite}
% \usepackage[margin=1.1in]{geometry}

\begin{document}
\title{6.867 Final Project: Why Do We Need Complex Networks}
\author{Michele Pratusevich}
\date{\today}
\maketitle

\begin{abstract}
Something here. Do I even need this?
\end{abstract}

\section{Introduction}

The advent of perceptrons in machine learning classification tasks were developed as a supervised learning method for classification of linearly separable datasets in the 1950s, with inputs from scientists in neuroscience and artificial intelligence \cite{rosenblatt_perceptron:_1958}. The slow development from single-layer perceptrons to multi-layer perceptrons and kernel perceptrons \cite{aizerman_theoretical_1964} and the increasing popularity of kernelized support vector machines for non-linear classification meant that neural networks were put on the back burner of research for a few decades.

A neural network success story in the computer vision community is the development of a back-propagation convolutional neural network (CNN) for handwritten digit recognition \cite{lecun_handwritten_1990}. The original data used was approximately 10,000 handwritten digits, classified into one of 10 categories (one category for each digit). This original dataset was later expanded into the MNIST handwritten digit database with over 60,000 examples \cite{lecun_gradient-based_1998}, \cite{li_deng_mnist_2012}. The network used by LeCunn et. al. used the idea of convolution layers - described essentially as features extracted from parts of the image by kernels of varying sizes, generally getting smaller and smaller. The general idea is that these $n \times n$ kernels will `find' specific features in the image. In between, there are layers that perform averaging or weighting of the features that it receives. The final layer is fully-connected to the receptive fields of the previous layer, meaning the output is a vector that predicts which of the 10 classes the digit falls into. Improvements to this network have been made incrementally over time, but the error rates on the test set for this initial network was $3.7$ percent. 

The interesting insights from LeCun's simplified architecture \cite{lecun_handwritten_1990} compared to a previous network that had more connections (also designed by LeCun) \cite{lecun_backpropagation_1989} were two-fold: (1) that images could be fed directly into the neural network rather than extracting features, then feeding the features into the network, and (2) knowing a-priori the nature of the classification task can give hints about how to simplify the network to contain fewer parameters to optimize over. In general, these ideas of simplification of neural networks can be applied to larger and more complex neural networks \cite{lecun_optimal_1989}. 

Since LeCun's successful use of CNNs for handwritten digit classification, CNNs became a popular technique for completing computer vision tasks. Krizhevsky et. al. \cite{krizhevsky_imagenet_2012} constructed an 8-layer CNN that achieved 17 percent top-5 error rates on the ImageNet database \cite{russakovsky_imagenet_2014}, taking advantage of optimizations using GPUs and a few other numerical improvements cited in the paper. Since then, very deep CNNs have been used for nearly every computer vision task with a standard dataset, and countless other non-standardized tasks. Since the original AlexNet was published, fine-tunings of the original network, in addition to changes to the architecture, were performed with the hopes of creating a neural network that can be used as a generalized feature extractor for all kinds of image tasks \cite{donahue_decaf:_2014}. Software packages to facilitate this and the use of GPUs for speed-up were created \cite{jia_caffe:_2014}. 

One big problem with using these extremely deep CNNs for feature extraction, fine-tuning, and general image classification tasks is the time needed to train these networks. With GPU speedups and optimized code, this task is made easier, but it still takes approximately 3 days on a GPU to train on the entirety of the ImageNet database (6 million images). Fine-tuning for a specific task using a pre-trained network takes less time, but still requires significant time. Another big problem with deep CNNs for vision tasks is the need for extremely large quantities of labeled training data. For a large network with 10000s of parameters, having only 50 labeled training images is not enough to optimize over. AlexNet was only able to train from scratch because the training set for ImageNet is incredibly large. 

Training neural networks still requires the fine-tuning of hyperparameters dictating the learning rate and the weight decay, but some have offered suggestions for how to improve back-propagation and the choosing of hyperparameters \cite{bottou_large-scale_2010}. However, simultaneously, there have been a number of papers questioning the need for the depth and complexity of the recently-used CNNs \cite{ba_deep_2013}. LeCun provided suggestions for the simplification of neural networks when working with the original handwritten digit classification network \cite{lecun_optimal_1989}.

In this project, I will explore the idea of simplified CNNs for the scene recognition vision task, drawing on ideas from recent literature about the effectiveness of using pre-trained networks, simplifying existing networks, and using extracted features to do various tasks. 

\section{Dataset}

The task of scene recognition 

% * CNNs have proven effective in image classification because of the ability to learn high-level features
% * CNN is like a multiclass perceptron?
% 	* start with a single multiclass logistic regression classifier in caffe on SUN
% * http://groups.csail.mit.edu/vision/SUN/
% * neural networks are really networks of logistic regression things
% * multiclass logistic regression on SUN database

% * can use the dataset that is already on aditya's folder: 
% /data/vision/torralba/aditya_datasets/data/SUN397/SUN397

% 1. simple logistic regression classifier: network example given here http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html

\section{Method}

* had to install caffe
* mostly used resources online, except the built-in ATLAS on my OSX machine was not working, so had to add openblas to teh list of brew packages needed to be edited / installed / compiled

\section{Experiments}



\bibliographystyle{plain}
\bibliography{6867project}

\end{document}